use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::fs;
use std::path::PathBuf;

#[derive(Serialize, Deserialize, Default)]
struct InvertedIndexData {
    term_to_chunks: HashMap<String, Vec<String>>,
    chunk_tokens: HashMap<String, HashMap<String, u32>>,
    avg_doc_length: f64,
}

pub struct InvertedIndexInner {
    index_path: PathBuf,
    term_to_chunks: HashMap<String, HashSet<String>>,
    chunk_tokens: HashMap<String, HashMap<String, u32>>,
    total_token_count: u64,
}

impl InvertedIndexInner {
    pub fn new(index_path: PathBuf) -> Self {
        Self {
            index_path,
            term_to_chunks: HashMap::new(),
            chunk_tokens: HashMap::new(),
            total_token_count: 0,
        }
    }

    pub fn load(&mut self) -> Result<()> {
        if !self.index_path.exists() {
            return Ok(());
        }

        let content = fs::read_to_string(&self.index_path)?;
        let data: InvertedIndexData = serde_json::from_str(&content)?;

        self.term_to_chunks.clear();
        for (term, chunk_ids) in data.term_to_chunks {
            self.term_to_chunks
                .insert(term, chunk_ids.into_iter().collect());
        }

        self.chunk_tokens.clear();
        self.total_token_count = 0;
        for (chunk_id, tokens) in data.chunk_tokens {
            for count in tokens.values() {
                self.total_token_count += *count as u64;
            }
            self.chunk_tokens.insert(chunk_id, tokens);
        }

        Ok(())
    }

    pub fn save(&self) -> Result<()> {
        if let Some(parent) = self.index_path.parent() {
            fs::create_dir_all(parent)?;
        }

        let mut data = InvertedIndexData {
            term_to_chunks: HashMap::new(),
            chunk_tokens: self.chunk_tokens.clone(),
            avg_doc_length: self.get_avg_doc_length(),
        };

        for (term, chunk_ids) in &self.term_to_chunks {
            data.term_to_chunks
                .insert(term.clone(), chunk_ids.iter().cloned().collect());
        }

        let json = serde_json::to_string(&data)?;
        fs::write(&self.index_path, json)?;

        Ok(())
    }

    pub fn add_chunk(&mut self, chunk_id: &str, content: &str) {
        let tokens = self.tokenize(content);
        let mut term_freq: HashMap<String, u32> = HashMap::new();

        for token in &tokens {
            *term_freq.entry(token.clone()).or_insert(0) += 1;

            self.term_to_chunks
                .entry(token.clone())
                .or_insert_with(HashSet::new)
                .insert(chunk_id.to_string());
        }

        self.chunk_tokens.insert(chunk_id.to_string(), term_freq);
        self.total_token_count += tokens.len() as u64;
    }

    pub fn remove_chunk(&mut self, chunk_id: &str) -> bool {
        let tokens = match self.chunk_tokens.remove(chunk_id) {
            Some(t) => t,
            None => return false,
        };

        for (token, count) in &tokens {
            self.total_token_count = self.total_token_count.saturating_sub(*count as u64);

            if let Some(chunks) = self.term_to_chunks.get_mut(token) {
                chunks.remove(chunk_id);
                if chunks.is_empty() {
                    self.term_to_chunks.remove(token);
                }
            }
        }

        true
    }

    pub fn search(&self, query: &str) -> Vec<(String, f64)> {
        let query_tokens = self.tokenize(query);
        if query_tokens.is_empty() {
            return Vec::new();
        }

        let mut candidate_chunks: HashSet<String> = HashSet::new();
        for token in &query_tokens {
            if let Some(chunks) = self.term_to_chunks.get(token) {
                for chunk_id in chunks {
                    candidate_chunks.insert(chunk_id.clone());
                }
            }
        }

        let k1: f64 = 1.2;
        let b: f64 = 0.75;
        let n = self.chunk_tokens.len() as f64;
        let avg_doc_length = self.get_avg_doc_length();

        let mut scores: Vec<(String, f64)> = Vec::new();

        for chunk_id in candidate_chunks {
            let term_freq = match self.chunk_tokens.get(&chunk_id) {
                Some(tf) => tf,
                None => continue,
            };

            let doc_length: u32 = term_freq.values().sum();
            let mut score: f64 = 0.0;

            for term in &query_tokens {
                let tf = *term_freq.get(term).unwrap_or(&0) as f64;
                if tf == 0.0 {
                    continue;
                }

                let df = self
                    .term_to_chunks
                    .get(term)
                    .map(|s| s.len())
                    .unwrap_or(0) as f64;
                let idf = ((n - df + 0.5) / (df + 0.5) + 1.0).ln();

                let tf_norm =
                    (tf * (k1 + 1.0)) / (tf + k1 * (1.0 - b + b * (doc_length as f64 / avg_doc_length)));
                score += idf * tf_norm;
            }

            if score > 0.0 {
                scores.push((chunk_id, score));
            }
        }

        let max_score = scores.iter().map(|(_, s)| *s).fold(1.0_f64, f64::max);
        for (_, score) in &mut scores {
            *score /= max_score;
        }

        scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
        scores
    }

    pub fn has_chunk(&self, chunk_id: &str) -> bool {
        self.chunk_tokens.contains_key(chunk_id)
    }

    pub fn clear(&mut self) {
        self.term_to_chunks.clear();
        self.chunk_tokens.clear();
        self.total_token_count = 0;
    }

    pub fn document_count(&self) -> usize {
        self.chunk_tokens.len()
    }

    fn get_avg_doc_length(&self) -> f64 {
        let count = self.chunk_tokens.len();
        if count > 0 {
            self.total_token_count as f64 / count as f64
        } else {
            100.0
        }
    }

    fn tokenize(&self, text: &str) -> Vec<String> {
        text.to_lowercase()
            .chars()
            .map(|c| if c.is_alphanumeric() { c } else { ' ' })
            .collect::<String>()
            .split_whitespace()
            .filter(|t| t.len() > 2)
            .map(|s| s.to_string())
            .collect()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    #[test]
    fn test_inverted_index_basic() {
        let dir = tempdir().unwrap();
        let index_path = dir.path().join("inverted-index.json");

        let mut index = InvertedIndexInner::new(index_path);

        index.add_chunk("chunk1", "function handleError throws exception");
        index.add_chunk("chunk2", "class UserController handles requests");
        index.add_chunk("chunk3", "error logging and debugging");

        assert_eq!(index.document_count(), 3);

        let results = index.search("error handling");
        assert!(!results.is_empty());

        let chunk_ids: Vec<&str> = results.iter().map(|(id, _)| id.as_str()).collect();
        assert!(chunk_ids.contains(&"chunk1") || chunk_ids.contains(&"chunk3"));
    }

    #[test]
    fn test_inverted_index_remove() {
        let dir = tempdir().unwrap();
        let index_path = dir.path().join("inverted-index.json");

        let mut index = InvertedIndexInner::new(index_path);

        index.add_chunk("chunk1", "function handleError");
        index.add_chunk("chunk2", "class UserController");

        assert_eq!(index.document_count(), 2);

        index.remove_chunk("chunk1");
        assert_eq!(index.document_count(), 1);
        assert!(!index.has_chunk("chunk1"));
        assert!(index.has_chunk("chunk2"));
    }

    #[test]
    fn test_inverted_index_persistence() {
        let dir = tempdir().unwrap();
        let index_path = dir.path().join("inverted-index.json");

        {
            let mut index = InvertedIndexInner::new(index_path.clone());
            index.add_chunk("chunk1", "function handleError throws exception");
            index.save().unwrap();
        }

        {
            let mut index = InvertedIndexInner::new(index_path);
            index.load().unwrap();
            assert_eq!(index.document_count(), 1);
            assert!(index.has_chunk("chunk1"));
        }
    }
}
